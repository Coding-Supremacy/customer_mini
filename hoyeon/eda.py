import streamlit as st
import pandas as pd
import plotly.express as px
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ë³€ê²½í•˜ì—¬ ë¹ ë¥´ê²Œ ë¡œë”©ë¨
@st.cache_resource
def load_model():
    model_name = "EleutherAI/polyglot-ko-1.3b"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')
    return tokenizer, model

tokenizer, model = load_model()

# ì•± íƒ€ì´í‹€
st.title("ğŸ“Š ê³ ê° ë°ì´í„° í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ AI")

# ë°ì´í„° ë¡œë“œ (ì‹¤ì œ íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)
df = pd.read_csv("hoyeon/í´ëŸ¬ìŠ¤í„°ë§ê³ ê°ë°ì´í„°_4.csv")

# ì›ë³¸ ë°ì´í„° ë³´ê¸°
st.subheader("ğŸ“Œ ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
st.dataframe(df.head())

# í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ìš”ì•½ (ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
cluster_summary = df.groupby('ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ (Customer Segment)').agg({
    'ì—°ë ¹': 'mean',
    'ì•„ì´ë”” (User ID)': 'count'
}).reset_index().rename(columns={
    'ì—°ë ¹': 'í‰ê·  ì—°ë ¹',
    'ì•„ì´ë”” (User ID)': 'ê³ ê° ìˆ˜'
})

cluster_summary.rename(columns={'ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ (Customer Segment)': 'ê³ ê° ì„¸ê·¸ë¨¼íŠ¸'}, inplace=True)

# ìš”ì•½ ë°ì´í„° í‘œì‹œ
st.subheader("ğŸ“ˆ í´ëŸ¬ìŠ¤í„°ë§ ìš”ì•½ ë°ì´í„°")
st.dataframe(cluster_summary)

# í´ëŸ¬ìŠ¤í„°ë³„ ê³ ê° ìˆ˜ ê·¸ë˜í”„
fig = px.bar(cluster_summary, x='ê³ ê° ì„¸ê·¸ë¨¼íŠ¸', y='ê³ ê° ìˆ˜',
             title='ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ê³ ê° ìˆ˜', color='ê³ ê° ì„¸ê·¸ë¨¼íŠ¸')
st.plotly_chart(fig, use_container_width=True)

# AI ë¶„ì„ ë²„íŠ¼
if st.button('ğŸ¤– AIë¡œ í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ë¶„ì„í•˜ê¸°'):

    summary_text = ""
    for _, row in cluster_summary.iterrows():
        summary_text += (f"[{row['ê³ ê° ì„¸ê·¸ë¨¼íŠ¸']}] ì„¸ê·¸ë¨¼íŠ¸ì˜ í‰ê·  ì—°ë ¹ì€ {row['í‰ê·  ì—°ë ¹']:.1f}ì„¸ì´ê³ , "
                         f"ì „ì²´ ê³ ê° ìˆ˜ëŠ” {row['ê³ ê° ìˆ˜']}ëª…ì…ë‹ˆë‹¤.\n")

    prompt = f"""
    ë‹¤ìŒì€ ê³ ê° ë°ì´í„°ë¥¼ í´ëŸ¬ìŠ¤í„°ë§í•œ ê²°ê³¼ì…ë‹ˆë‹¤:

    {summary_text}

    ê° ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ íŠ¹ì„±ì„ ê°„ëµíˆ ë¶„ì„í•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    """

    with st.spinner("AI ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        inputs = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(
            inputs,
            max_length=300,
            do_sample=False,
            early_stopping=True
        )
        analysis = tokenizer.decode(outputs[0], skip_special_tokens=True)

    st.subheader("âœ… AI ë¶„ì„ ê²°ê³¼")
    st.write(analysis)
